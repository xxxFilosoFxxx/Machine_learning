{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "        a, b = itertools.tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    def __init__(self):\n",
    "        self.iterations = 100  # количество итераций для градиентного спуска\n",
    "        self.alpha = 0.1  # скорость обучения\n",
    "        self.lamda = 0.5  # параметр для регуляризации\n",
    "        self.n = 4  # слоев в нейросети\n",
    "        self.m = [28, 3, 3, 1]  # массив числа нейронов дл каждого слоя\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "            \n",
    "    @staticmethod\n",
    "    def deriv_sigmoid(X):\n",
    "        fx = sigmoid(X)\n",
    "        return fx * (1 - fx)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_loss(Y, theta):\n",
    "        return ((Y - theta) ** 2).mean()\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        out = np.exp(X)\n",
    "        return out/np.sum(out)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(Y, theta):\n",
    "        return -np.sum(Y * np.log(theta)) \n",
    "\n",
    "    \n",
    "    def backpropagation(self, deriv_grad_func, y, x, weights, theta, R):\n",
    "        d_ypred = -2 * (y - theta[-1])\n",
    "        sum_h = 0\n",
    "        sum_old_h = 0\n",
    "        old_w = list()\n",
    "        for i in range(len(theta) - 1, 0, -1):\n",
    "            if i == len(theta) - 1:\n",
    "                old_w = weights[i]\n",
    "                for k in range(len(theta[i])):\n",
    "                    for z in range(len(weights[i][k])):\n",
    "                        sum_h += weights[i][k][z] * theta[i - 1][z]\n",
    "                    sum_h += R[i][k]\n",
    "                    for z in range(len(weights[i][k])):\n",
    "                        weights[i][k][z] -= self.alpha * d_ypred * theta[i - 1][z] * deriv_grad_func(sum_h)\n",
    "                    R[i][k] -= self.alpha * d_ypred * deriv_grad_func(sum_h)\n",
    "                    sum_h = 0\n",
    "            else:\n",
    "                present_w = weights[i]\n",
    "                for k in range(len(theta[i])):\n",
    "                    new_w = np.sum(old_w[k])\n",
    "                    for z in range(len(old_w[k])):\n",
    "                        sum_old_h += old_w[k][z] * theta[i][z]\n",
    "                    for z in range(len(weights[i][k])):\n",
    "                        sum_h += weights[i][k][z] * theta[i - 1][z]\n",
    "                    sum_h += R[i][k]\n",
    "                    for z in range(len(weights[i][k])):\n",
    "                        weights[i][k][z] -= self.alpha * d_ypred * new_w * deriv_grad_func(sum_old_h) * \n",
    "                                            theta[i - 1][z] * deriv_grad_func(sum_h)\n",
    "                    R[i][k] -= self.alpha * d_ypred * new_w * deriv_grad_func(sum_old_h) * deriv_grad_func(sum_h)\n",
    "                    sum_old_h = 0\n",
    "                    sum_h = 0\n",
    "                old_w = present_w\n",
    "        for k in range(len(theta[0])):\n",
    "            new_w = np.sum(old_w[k])\n",
    "            for z in range(len(old_w[k])):\n",
    "                sum_old_h += old_w[k][z] * theta[0][z]\n",
    "            for z in range(len(weights[0][k])):\n",
    "                sum_h += weights[0][k][z] * x[k][z]\n",
    "            sum_h += R[0][k]\n",
    "            for z in range(len(weights[0][k])):\n",
    "                weights[0][k][z] -= self.alpha * d_ypred * new_w * deriv_grad_func(sum_old_h) * \n",
    "                                    x[k][z] * deriv_grad_func(sum_h)\n",
    "            R[0][k] -= self.alpha * d_ypred * new_w * deriv_grad_func(sum_old_h) * deriv_grad_func(sum_h)\n",
    "            sum_old_h = 0\n",
    "            sum_h = 0\n",
    "        \n",
    "        \n",
    "    def forward(self, grad_func, x, weights, theta, R):\n",
    "        sum_h = 0\n",
    "        for k in range(len(theta[0])):\n",
    "            for z in range(len(weights[0][k])):\n",
    "                sum_h += weights[0][k][z] * x[k][z]\n",
    "            sum_h += R[0][k]\n",
    "            theta[0][k] = grad_func(sum_h)\n",
    "            sum_h = 0\n",
    "        for i in range(1, len(theta)):\n",
    "            for k in range(len(theta[i])):\n",
    "                for z in range(len(weights[i][k])):\n",
    "                    sum_h += weights[i][k][z] * theta[i - 1][z]\n",
    "                sum_h += R[i][k]\n",
    "                theta[i][k] = grad_func(sum_h)\n",
    "                sum_h = 0\n",
    "        return theta[-1]\n",
    "        \n",
    "\n",
    "    def gradient_descent(self, data, y_true, deriv_grad_func=deriv_sigmoid,\n",
    "                         grad_func=sigmoid, loss_func=mse_loss):\n",
    "        theta = [0] * self.n\n",
    "        weights = [0] * self.n\n",
    "        R = [0] * self.n\n",
    "        j = 0\n",
    "        for i in self.m:\n",
    "            theta[j] = np.random.randn(i, 1)\n",
    "            R[j] = np.full(i, 0).reshape(-1, 1)\n",
    "            j += 1\n",
    "            \n",
    "        j = 1\n",
    "        weights[0] = np.random.randn(28, self.m[0])  # первый параметр равен размерности X\n",
    "        for z1, z2 in pairwise(self.m):\n",
    "            weights[j] = np.random.randn(z1, z2)\n",
    "            j += 1\n",
    "\n",
    "        reg_theta = lambda theta, lamda, k: (lamda / len(theta)) * theta[k]\n",
    "        \n",
    "        for i in range(len(theta)):\n",
    "            for k in range(len(theta[i])):  # регуляризация каждой theta\n",
    "                R[i][k] = reg_theta(theta[i], self.lamda, k)\n",
    "                \n",
    "        for iteration in range(self.iterations):  # обновление параметров\n",
    "            for x, y in zip(data, y_true):\n",
    "                y_pred = self.forward(grad_func, x, weights, theta, R)\n",
    "                self.backpropagation(deriv_grad_func, y, x, weights, theta, R)\n",
    "            if iteration % 10 == 0:\n",
    "                loss = loss_func(y_true, y_pred)\n",
    "                print(\"Epoch %d loss: %.3f\" % (iteration, loss))\n",
    "        \n",
    "        # return theta[-1], loss_func(y_true, theta[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [8, 2, 3, 1]\n",
    "w = [0] * 4\n",
    "j = 1   \n",
    "w[0] = np.random.randn(8, m[0])\n",
    "for z1, z2 in pairwise(m):\n",
    "    w[j] = np.random.randn(z1, z2)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-6.65642245e-01,  4.12316271e-01,  3.70166096e-01,\n",
       "         -6.05833387e-01, -2.49084505e+00,  4.97623854e-01,\n",
       "          2.80971588e+00, -6.26117224e-01],\n",
       "        [ 6.79219248e-01,  1.35254801e+00,  1.08972867e+00,\n",
       "         -1.58512950e+00,  5.31412591e-01, -9.11538278e-01,\n",
       "          9.86457354e-02,  6.90513701e-01],\n",
       "        [-1.12758521e+00, -8.72653456e-01,  1.70353701e+00,\n",
       "          4.83501117e-01, -8.06697869e-02,  1.65100776e-01,\n",
       "         -1.13886541e-01, -2.51882660e-01],\n",
       "        [-2.68177174e-01,  9.28912978e-01, -2.81097004e-01,\n",
       "         -1.52273657e+00,  1.09795046e+00, -6.72331838e-01,\n",
       "          1.11201469e+00, -1.09565737e-01],\n",
       "        [ 1.52122643e+00, -1.09253714e+00, -1.78631857e-01,\n",
       "         -1.86107178e-01,  1.14485692e-01, -8.17598583e-01,\n",
       "          5.39964178e-03, -6.91309089e-01],\n",
       "        [ 5.45743108e-01,  1.60404856e+00, -6.53002219e-01,\n",
       "          1.76528664e+00, -9.19805729e-04, -3.25607178e-01,\n",
       "          2.18638100e-01, -1.04023705e+00],\n",
       "        [-4.40432876e-01, -1.19002352e+00, -7.36684516e-02,\n",
       "         -1.55203815e+00,  2.25208559e+00,  6.35527464e-01,\n",
       "          4.74690141e-01,  9.03109369e-01],\n",
       "        [-4.04360851e-01,  7.58193316e-01, -1.44282269e+00,\n",
       "         -3.93079357e-01, -1.74070633e+00, -2.46111312e+00,\n",
       "         -7.48523487e-01,  2.71809516e-01]]),\n",
       " array([[-0.45003085, -0.34110853],\n",
       "        [-0.78253987, -0.53673928],\n",
       "        [-0.53343052,  0.15318595],\n",
       "        [ 0.96714698,  0.45105683],\n",
       "        [-0.53232404,  0.0104792 ],\n",
       "        [ 0.11699057,  0.87509726],\n",
       "        [ 0.82337442, -0.95108817],\n",
       "        [-0.18928803, -0.56320951]]),\n",
       " array([[ 0.02121744,  0.03538321, -0.44522331],\n",
       "        [-0.61740874, -0.67577303,  0.65279571]]),\n",
       " array([[ 0.11632182],\n",
       "        [ 0.80052557],\n",
       "        [-0.50330603]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
